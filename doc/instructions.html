<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Image classification practical (CNN version)</title>
  <link rel="stylesheet" href="base.css" />
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
  </script>
  <script type="text/javascript"
          src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
  <!-- Code highlights -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.1.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.1.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
</head>
<body>
<h1 id="vgg-convolutional-neural-networks-practical">VGG Convolutional Neural Networks Practical</h1>
<p><em>By Andrea Vedaldi and Andrew Zisserman</em></p>
<p>This is an <a href="http://www.robots.ox.ac.uk/~vgg">Oxford Visual Geometry Group</a> computer vision practical, authored by <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a> and Andrew Zisserman (Release 2015a).</p>
<p><img height=400px src="images/cover.png" alt="cover"/></p>
<p><em>Convolutional neural networks</em> are an important class of learnable representations applicable, among others, to numerous computer vision problems. Deep CNNs, in particular, are composed of several layers of processing, each involving linear as well as non-linear operators, that are learned jointly, in an end-to-end manner, to solve a particular tasks. These methods are now the dominant approach for feature extraction from audiovisual and textual data.</p>
<p>This practical explores the basics of learning (deep) CNNs. The first part introduces typical CNN building blocks, such as ReLU units and linear filters, with a particular emphasis on understanding back-propagation. The second part looks at learning two basic CNNs. The first one is a simple non-linear filter capturing particular image structures, while the second one is a network that recognises typewritten characters (using a variety of different fonts). These examples illustrate the use of stochastic gradient descent with momentum, the definition of an objective function, the construction of mini-batches of data, and data jittering. The last part shows how powerful CNN models can be downloaded off-the-shelf and used directly in applications, bypassing the expensive training process.</p>
<div class="toc">
<ul>
<li><a href="#vgg-convolutional-neural-networks-practical">VGG Convolutional Neural Networks Practical</a><ul>
<li><a href="#getting-started">Getting started</a></li>
<li><a href="#part1">Part 2: Learning a CNN for text deblurring</a><ul>
<li><a href="#part1.1">Part 2.1: preparing the data</a></li>
</ul>
</li>
<li><a href="#part-22-preparing-the-network">Part 2.2: preparing the network</a></li>
<li><a href="#links-and-further-work">Links and further work</a></li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
<li><a href="#history">History</a></li>
</ul>
</li>
</ul>
</div>
<p>$$
   \newcommand{\bx}{\mathbf{x}}
   \newcommand{\by}{\mathbf{y}}
   \newcommand{\bz}{\mathbf{z}}
   \newcommand{\bw}{\mathbf{w}}
   \newcommand{\cP}{\mathcal{P}}
   \newcommand{\cN}{\mathcal{N}}
   \newcommand{\vc}{\operatorname{vec}}
$$</p>
<h2 id="getting-started">Getting started</h2>
<p>Read and understand the <a href="../overview/index.html#installation">requirements and installation instructions</a>. The download links for this practical are:</p>
<ul>
<li>Code and data: <a href="http://www.robots.ox.ac.uk/~vgg/share/practical-cnn-2015a.tar.gz">practical-cnn-2015a.tar.gz</a></li>
<li>Code only: <a href="http://www.robots.ox.ac.uk/~vgg/share/practical-cnn-2015a-code-only.tar.gz">practical-cnn-2015a-code-only.tar.gz</a></li>
<li>Data only: <a href="http://www.robots.ox.ac.uk/~vgg/share/practical-cnn-2015a-data-only.tar.gz">practical-cnn-2015a-data-only.tar.gz</a></li>
<li><a href="https://github.com/vedaldi/practical-cnn">Git repository</a> (for lab setters and developers)</li>
</ul>
<p>After the installation is complete, open and edit the script <code>exercise1.m</code> in the MATLAB editor. The script contains commented code and a description for all steps of this exercise, for <a href="#part1">Part I</a> of this document. You can cut and paste this code into the MATLAB window to run it, and will need to modify it as you go through the session. Other files <code>exercise2.m</code>, <code>exercise3.m</code>, and <code>exercise4.m</code> are given for <a href="#part2">Part II</a>, <a href="#part3">III</a>, and <a href="part4">IV</a>.</p>
<p>Each part contains several <strong>Questions</strong> (that require pen and paper) and <strong>Tasks</strong> (that require experimentation or coding) to be answered/completed before proceeding further in the practical.</p>
<h2 id="part1">Part 2: Learning a CNN for text deblurring</h2>
<p>In this part of the practical, we will learn a CNN that generates an image instead of performing classification. This is a simple demonstration of how CNNs can be used well beyond classification tasks.</p>
<p>The goal of the exercise is to learn a function that takes a blurred text as input and produces a crispier version as output. This problem is generally known as <em>deblurring</em> and is widely studied in computer vision, image processing, and computational photography. Here, instead of constructing a deblurring filter from first principles, we simply learn it from data. A key advantage is that the learned function can incorporate a significant amount of domain-specifc knowledge and perform particularly well on the particualr domain of interst.</p>
<p>Start by opening in your MATLAB editor <code>exercise2.m</code>.</p>
<h3 id="part1.1">Part 2.1: preparing the data</h3>
<p>The first task is to load the training and validation data and to understand its format. The code responsible for loading such data is</p>
<pre><code class="matlab">imdb = load('data/text_imdb.mat') ;
</code></pre>

<p>The variable <code>imdb</code> is a structure containing $n$ images. The structure has the following fields:</p>
<ul>
<li><code>imdb.images.data</code>: a $64 \times 64 \times 1 \times n$ array of grayscale images.</li>
<li><code>imdb.images.label</code>: a $64 \times 64 \times 1 \times n$ of image "labels"; for this problem, a label is also a grayscale image.</li>
<li><code>imdb.images.set</code>: a $1 \times n$ vector containing a 1 for training images and an 2 for validation images.</li>
</ul>
<p>Each trainig datapoint is a blurred image of text (extracted from a scientific paper). Its "label" is the sharp version of the same image: learning the deblurring function is formulated as the problem of regressing the sharp image from the blurred one.</p>
<p>Run the following code, which displays the first image and corresponding label in the dataset:</p>
<pre><code class="matlab">figure(100) ; clf ;

subplot(1,2,1) ; imagesc(imdb.images.data(:,:,:,1)) ;
axis off image ; title('input (blurred)') ;

subplot(1,2,2) ; imagesc(imdb.images.label(:,:,:,1)) ;
axis off image ; title('desired output (sharp)') ;

colormap gray ;
</code></pre>

<p>It should produce the following output:</p>
<p><img alt="Data example" src="images/text.png" /></p>
<p>The images are split in 75% training images and 25% validation images, as indicated by the flag <code>imdb.images.set</code>.</p>
<blockquote>
<p><strong>Task:</strong> make sure you understand the data format. How many training and validation images are there? What is the resolution of the individual images?</p>
</blockquote>
<h2 id="part-22-preparing-the-network">Part 2.2: preparing the network</h2>
<p>The next task is to construct a network <code>net</code> and initialize its weights. We are going to use the SimpleNN wrapper in MatConvNet (more complex architectures can be implemented using the DagNN wrapper).</p>
<p>A network is simply a sequence of functions. We initialize this as the empty list:</p>
<pre><code class="matlab">net.layers = { } ;
</code></pre>

<p>Layers need to be listed in <code>net.layers</code> in the order of execution, from first to last. For example, the following code adds the first layer of the network, a convolutional one:</p>
<pre><code class="matlab">net.layers{end+1} = struct(...
  'name', 'conv1', ...
  'type', 'conv', ...
  'weights', {xavier(3,3,1,32)}, ...
  'pad', 1, ...
  'stride', 1, ...
  'learningRate', [1 1], ...
  'weightDecay', [1 0]) ;
</code></pre>

<p>The <code>name</code> field specifies a name for the layer, useful for debugging but otherwise arbitrary. The <code>type</code> field specifised which type of layer we want, in this case convolution. The <code>weights</code> layer is a cell array containing two arrays, one for the filters and one for the biases. In this example, the filter array has dimension $3 \times 3 \times 1 \times 32$, which is a filter bank of 32 filters, with $3\times 3$ spatial support, and operating on 1 input channel. The biases array has dimension $32 \times 1$ as it specifies one bias per filter. These two arrays are initialize randomly by the <code>xavier</code> function, using Xiavier's method.</p>
<p>The <code>pad</code> and <code>stride</code> options specify the filter padding and stride. Here the stride is dense (one pixel) and there is a one pixel padding. In this manne, the output tensor has exactly the same spatial dimensions of the input one.</p>
<p>Finally, the <code>learningRate</code> and <code>weightDecay</code> options specify filter-specific multipliers for the learning rate and weight decay for the filters and the biases.</p>
<p>The next layer to be added is simply a ReLU activation function, which is non-linear:</p>
<pre><code class="matlab">net.layers{end+1} = struct(...
  'name', 'relu1', ...
  'type', 'relu') ;
</code></pre>

<p>The architecture consists of a number of such convolution-ReLU blocks.</p>
<blockquote>
<p><strong>Question:</strong> The last block, generating the final image, is only convolutional, and it has exactly one filter. Why?</p>
</blockquote>
<p>This is still not sufficient to learn the model. We need in facto a loss function too. For this task, we use the Euclidean distance between the generated image and the desired one. This is implemented by the <code>pdist</code> block.</p>
<pre><code class="matlab">net.layers{end+1} = struct(...
  'name', 'loss', ...
  'type', 'pdist', ...
  'p', 2, ...
  'aggregate', true, ...
  'instanceWeights', 1/(64*64)) ;
</code></pre>

<p>Here <code>p</code> is the exponent of the p-distance (set to 2 for Eucliden), <code>aggregate</code> means that the individual squared pixel differences should be summed in a grand total for the whole image, and <code>instanceWeights</code> is a (homogeneous) scaling factors for all the pixels, set to the inverse area of an image. The latter two options make it so that the squared difference are averaged across pixels, resulting in a normalized Euclidean distance between generated and target images.</p>
<p>We add one last parameter</p>
<pre><code class="matlab">net.meta.inputSize = [64 64 1 1] ;
</code></pre>

<p>which  specifies the expected dimensions of the network input. Finally, we call the <code>vl_simplenn_tidy()</code> function to check the network parameters and fill in default values for any parameter that we did not specify yet:</p>
<pre><code class="matlab">net = vl_simplenn_tidy(net) ;
</code></pre>

<p>Finally, we display the parameters of the network just created:</p>
<pre><code class="matlab">vl_simplenn_display(net) ;
</code></pre>

<blockquote>
<p><strong>Questions:</strong> Look carefully at the generated table and answer the following questions:</p>
<ol>
<li>How many layers are in this network?</li>
<li>What is the sampling density of each layer?</li>
<li>What is the dimensionality of each intermediate feature map? How is that related with the number of filters in each convolutional layer?</li>
<li>Is there a special relationship between number of channels in a feature map and a certain dimension of the following filter bank?</li>
<li>What is the receptive field size of each feature? Is that proportionate to the size of a character?</li>
</ol>
</blockquote>
<h2 id="links-and-further-work">Links and further work</h2>
<ul>
<li>The code for this practical is written using the software package <a href="http://www.vlfeat.org/matconvnet">MatConvNet</a>. This is a software library written in MATLAB, C++, and CUDA and is freely available as source code and binary.</li>
<li>The ImageNet model is the <em>VGG very deep 16</em> of Karen Simonyan and Andrew Zisserman.</li>
</ul>
<h2 id="acknowledgements">Acknowledgements</h2>
<ul>
<li>Beta testing by: Karel Lenc and Carlos Arteta.</li>
<li>Bugfixes/typos by: Sun Yushi.</li>
</ul>
<h2 id="history">History</h2>
<ul>
<li>Used in the Oxford AIMS CDT, 2015-16.</li>
<li>Used in the Oxford AIMS CDT, 2014-15.</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:lattice">
<p>A two-dimensional <em>lattice</em> is a discrete grid embedded in $R^2$, similar for example to a checkerboard.&#160;<a class="footnote-backref" href="#fnref:lattice" rev="footnote" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div></body>
</html>
